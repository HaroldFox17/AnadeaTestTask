{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-TGBH97yXB3"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data and explore the data format"
      ],
      "metadata": {
        "id": "IyTFBujV2zLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/train-v2.0.json')\n",
        "data = json.load(f)"
      ],
      "metadata": {
        "id": "kmcFMUvfzoRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdxTR2-A0AMb",
        "outputId": "fa2d63ea-2632-4d4e-eefe-06e19fa9086e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['version', 'data'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data['data']:\n",
        "    print(i.keys())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3MrSaDre1ngS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data['data']:\n",
        "    print(i[\"paragraphs\"][0].keys())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BlIWvioH18kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change data format to meet desired standard and split it on test and validation"
      ],
      "metadata": {
        "id": "H9HRZxBe4doN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are gonna be using models from simpletransformers library and finetuning them we will need to convert the data to the format library requires"
      ],
      "metadata": {
        "id": "Sr10M_FK4qvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also do train validation split at this point. Since data has texts batched by common theme we will try three approaches for split:\n",
        "\n",
        "*   Spliting batches\n",
        "*   Splitting within batches\n",
        "*   Splitting with no regard to batches\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m5Npvz_cmkp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "E5FlCe-EjiBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting_within_batches(data, test_size):\n",
        "    training_data = []\n",
        "    validation_data = []\n",
        "    for i in data['data']:\n",
        "        train, val = train_test_split(i['paragraphs'], test_size=test_size)\n",
        "        training_data += train\n",
        "        validation_data += val\n",
        "    return training_data, validation_data"
      ],
      "metadata": {
        "id": "OAyrGHB_6XRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting_batches(data, test_size):\n",
        "    training_data = []\n",
        "    validation_data = []\n",
        "    train, val = train_test_split(data[\"data\"], test_size=test_size)\n",
        "    for i in train:\n",
        "        training_data += i['paragraphs']\n",
        "    for i in val:\n",
        "        validation_data += i['paragraphs']\n",
        "    return training_data, validation_data"
      ],
      "metadata": {
        "id": "xGSWvfyOnnMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting_with_no_batches(data, test_size):\n",
        "    new_data = []\n",
        "    for i in data[\"data\"]:\n",
        "        new_data += i['paragraphs']\n",
        "    training_data, validation_data = train_test_split(new_data, test_size=test_size)\n",
        "    return training_data, validation_data"
      ],
      "metadata": {
        "id": "Zj9zwVkEoHFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = splitting_within_batches(data, 0.2)\n",
        "with open('training_data_within_batches.json', 'w') as f:\n",
        "  json.dump(training_data, f, ensure_ascii=False)\n",
        "with open('validation_data_within_batches.json', 'w') as f:\n",
        "  json.dump(validation_data, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "TmKWCcju-WIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = splitting_batches(data, 0.2)\n",
        "with open('training_data_batches.json', 'w') as f:\n",
        "  json.dump(training_data, f, ensure_ascii=False)\n",
        "with open('validation_data_batches.json', 'w') as f:\n",
        "  json.dump(validation_data, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "2nYVZwONkr44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = splitting_with_no_batches(data, 0.2)\n",
        "with open('training_data_with_no_batches.json', 'w') as f:\n",
        "  json.dump(training_data, f, ensure_ascii=False)\n",
        "with open('validation_data_with_no_batches.json', 'w') as f:\n",
        "  json.dump(validation_data, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "JJ-geLzOpXpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data is now list of dictionaries that meets data format described at [simpletransformers library docs](https://pypi.org/project/simpletransformers/0.26.0/#question-answering)"
      ],
      "metadata": {
        "id": "qQmX5xjd88Te"
      }
    }
  ]
}